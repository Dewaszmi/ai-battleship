These are the files containing the old PPO implementation in PyTorch. I leave them for legacy case, the model seemed to perform pretty well at smaller grid sizes (measured performance during final episodes of training was similiar to what you would expect from a human player), but apparently it didn't scale well, as training at the desired 10x10 grid size took a very long time.
